{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWi6v8VZcFR-",
    "nbpresent": {
     "id": "cc5e48d4-0c22-454b-94f6-59d0a2c24e44"
    }
   },
   "source": [
    "# NLP on Ted Talk transcripts\n",
    "\n",
    "slides and code located at\n",
    "\n",
    "https://github.com/1fmusic/jean_bartik_computing_symposium_rankin.git\n",
    "\n",
    "\n",
    "# Explore, Clean and Pre-process text\n",
    "In this notebook we will \n",
    "\n",
    "1. Clean\n",
    "2. Tokenize\n",
    "3. Stem/lemmatize\n",
    "4. Normalize (remove stopwords, unwanted characters, punctuation, lowercase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new conda environment with the correct packages \n",
    "\n",
    "To create a new environment in (ana)conda - which you should do for each project so you dont break stuff - download the `environment.yml` file and follow these directions. \n",
    "\n",
    "Open a conda prompt (windows) or a terminal window (linux/mac):<br>\n",
    "            `$ cd ~/Documents/path_where_i_put_the_yml_file/`<br>\n",
    "            `$ conda env create -f environment.yml`<br>\n",
    "\n",
    "Activate the environment (the name is in the yml file)<br>\n",
    "            `$ conda activate jbcs2020`<br>\n",
    "            `$ jupyter notebook` \n",
    "\n",
    "Then click on the jupyter notebook titled `ted_clean_jbcs.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tooa5FNZXLGN"
   },
   "source": [
    "# Install and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T19:38:48.243042Z",
     "start_time": "2017-11-15T19:38:48.173542Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "J-gCJUeOcFSC",
    "nbpresent": {
     "id": "e1369d89-1774-4a4b-9832-5fc4ba50e389"
    },
    "outputId": "707d622f-b117-4fa6-ea81-d25bbab58bb1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY The first time you use the environment, download these packages from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T19:38:48.243042Z",
     "start_time": "2017-11-15T19:38:48.173542Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "J-gCJUeOcFSC",
    "nbpresent": {
     "id": "e1369d89-1774-4a4b-9832-5fc4ba50e389"
    },
    "outputId": "707d622f-b117-4fa6-ea81-d25bbab58bb1"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oxCGx8LcFSE",
    "nbpresent": {
     "id": "e731e3dc-c274-45d9-b74e-dfd6ad3c9ed8"
    }
   },
   "source": [
    "# Import Data\n",
    "We import the csv of transcripts and URLs into a pandas dataframe. \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html\n",
    "\n",
    "https://chrisalbon.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:02:51.838596Z",
     "start_time": "2017-11-13T20:02:50.971925Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yz_5f-0ncFSG",
    "nbpresent": {
     "id": "1e3ce296-d7f0-429f-a15b-2f81b4a24578"
    }
   },
   "outputs": [],
   "source": [
    "talks = pd.read_csv('./data/ted_trans.csv', encoding = \"UTF-8\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "qzBCEeKan2MS",
    "outputId": "eedb7c4c-9bea-4ddd-87cb-6305b35d0d13"
   },
   "outputs": [],
   "source": [
    "# print the first 5 rows using pandas 'head()' method\n",
    "talks.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the transcript column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talks = talks.loc[:,'transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHK6cMuwcFSQ",
    "nbpresent": {
     "id": "d55d24e6-6e57-4392-b749-a68a5979946f"
    }
   },
   "source": [
    "TODO: print portions from 3 different transcripts (**talks**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "QFRsbmEvcFSQ",
    "nbpresent": {
     "id": "8f751ba7-805e-4b90-aae1-3276a9e4d882"
    },
    "outputId": "748c59db-4477-41d3-9124-1b6ea094f5c4"
   },
   "outputs": [],
   "source": [
    "talks[0][:521]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print talk 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksa5ULR0cFST"
   },
   "outputs": [],
   "source": [
    "##TODO print talk number 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y63-FRNAcFSW"
   },
   "outputs": [],
   "source": [
    "##TODO print another talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poq8C6WOcFSZ",
    "nbpresent": {
     "id": "3b999ca7-9b38-495e-897e-553bc7f7855e"
    }
   },
   "source": [
    "#### number of transcripts you want to analyze (also creates a list of numbers for iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poq8C6WOcFSZ",
    "nbpresent": {
     "id": "3b999ca7-9b38-495e-897e-553bc7f7855e"
    }
   },
   "outputs": [],
   "source": [
    "fileids = range(0,len(talks))\n",
    "fileids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWb53RsocFSh",
    "nbpresent": {
     "id": "4d2baf07-1299-42ef-9cd4-8152a1705473"
    }
   },
   "source": [
    "# Tokenize (split) into words\n",
    "Typically, you would just go straignt to word tokenization if you are planning to do topic modeling. There are MANY, MANY ways to tokenize text into words. I will just show a few, but feel free to explore the possibilities.\n",
    "\n",
    "## Method 1\n",
    "wordpunct_tokenize from NLTK\n",
    "splits the text into words and punctuaiton as separate tokens (this makes it easy to remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:05:30.057043Z",
     "start_time": "2017-11-13T20:05:29.977823Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "p2hHUkTvcFSi",
    "nbpresent": {
     "id": "29faa657-8c60-41dd-badd-8ef42b727c8d"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_talks = [nltk.wordpunct_tokenize(talks[fileid]) \\\n",
    "             for fileid in fileids]\n",
    "\n",
    "#to view a few\n",
    "print('\\n-----\\n'.join(nltk.wordpunct_tokenize(talks[0][500:560])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "di7kGdGaLS1O"
   },
   "source": [
    "## Method 2\n",
    "Word_tokenize from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL-IhFQ5otkj"
   },
   "outputs": [],
   "source": [
    "doc_words_word_tok = [nltk.word_tokenize(talks[fileid]) \\\n",
    "             for fileid in fileids]\n",
    "\n",
    "print('\\n-----\\n'.join(nltk.word_tokenize(talks[0][500:560])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pa3261ypcFSo",
    "nbpresent": {
     "id": "d2bade29-473c-431c-a8b9-e8a08f0f27b1"
    }
   },
   "source": [
    "# Normalization\n",
    "## Lemmatize\n",
    "\n",
    "+ A method for getting the word root.\n",
    "+ It will replace the ending with the correct letters instead of chopping it off like some of the stemming functions. This leaves us will a few non-stemmed words.  \n",
    "        i.e. children -> child,   capacities -> capacity, but also, unpredictability -> unpredictability\n",
    "        \n",
    "## Lowercase\n",
    "+ also lowercase using **.lower()** at the word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:07:23.996217Z",
     "start_time": "2017-11-13T20:06:44.952044Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Of27eaGOcFSp",
    "nbpresent": {
     "id": "f9e204aa-feb7-4869-a80a-ef3094215a45"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "my_text = \"With our capabilities, we will educate the children. They are all associated with various playgrounds.\"\n",
    "\n",
    "\n",
    "for word in nltk.wordpunct_tokenize(my_text):\n",
    "    print(word, lemmizer.lemmatize(word.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZDR_43mNHC4"
   },
   "source": [
    "## Stem\n",
    "Now we will see how stemming with the porter stemmer the tokenized words will cut off the word ending to get to the root. Now we get `recently -> recent`, but also `associated -> associ`.\n",
    "\n",
    "We can print out the original word next to the stemmed word to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXOAIwe0Na93"
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "my_text = \"With our capabilities, we will educate the children. They are all associated with various playgrounds.\"\n",
    "\n",
    "\n",
    "for word in nltk.wordpunct_tokenize(my_text):\n",
    "    print(word, stemmer.stem(word.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvHR7q7vcFSs",
    "nbpresent": {
     "id": "72c1add6-0d9e-4a13-9a3c-fe6982cbde25"
    }
   },
   "source": [
    "# Remove Stopwords, punctuation, or other non-letter/numbers\n",
    "+ NLTK has a set of common words that do not add any semantic information to our text, we will use this list and add our own items to it\n",
    "        + punctuation\n",
    "        + music notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibuQqobTcFSy",
    "nbpresent": {
     "id": "9e5c4811-1d57-44d9-afb6-a5b092aa60f7"
    }
   },
   "outputs": [],
   "source": [
    "stop = nltk.corpus.stopwords.words('english')\n",
    "stop[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbNtnp54cFS0",
    "nbpresent": {
     "id": "b74b3f51-371b-40ae-81a8-0a34d39939c2"
    }
   },
   "source": [
    "add our own terms or characters to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDCFJkR4cFS1",
    "nbpresent": {
     "id": "141b869c-74bc-4995-b27b-4a44d0393c64"
    }
   },
   "outputs": [],
   "source": [
    "stop += ['.', ',',':','...','!\"','?\"', \"'\", '\"',' - ',' — ',',\"','.\"',';','♫♫','♫']\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnJ2NwlocFS2",
    "nbpresent": {
     "id": "2533d426-28f1-4617-8db0-9a32431df333"
    }
   },
   "source": [
    "Write a function to remove the stop words from a document using our list. Print a few talks and see if there are still a few words in there that are not giving us any information. If so, add them to the **stop** list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phRRUgWGcFS4",
    "nbpresent": {
     "id": "ca884bd9-a8c3-4507-b307-41557024b15d"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkYz6sNxcFSZ",
    "nbpresent": {
     "id": "6ad747ed-188f-426c-a325-cc496cf41043"
    }
   },
   "source": [
    "# Non-speech sounds, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:04:46.891465Z",
     "start_time": "2017-11-13T20:04:46.883084Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "asDBpxnKcFSc",
    "nbpresent": {
     "id": "eeb71ea6-10a5-45ba-8f93-9f2b713a84f0"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove parethetical non-speech sounds from text using a regular expression\n",
    "clean_parens_talks= [re.sub(r'\\([^)]*\\)', ' ', talks[fileid]) for fileid in fileids]\n",
    "\n",
    "# print one talk\n",
    "clean_parens_talks[1][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talks[1][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kfz3tNyucFS6",
    "nbpresent": {
     "id": "053671ba-b59d-4142-9a04-1ab99329c1e0"
    }
   },
   "source": [
    "#  Define a cleaning function that combines the methods from above.\n",
    "1. clean (remove parentheticals)\n",
    "2. tokenize into words using wordpunct\n",
    "3. lowercase and remove stop words\n",
    "4. lemmatize or stem\n",
    "5. lowercase and remove stop words\n",
    "6. join the words back into a document and put into a list of cleaned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T23:03:29.257095Z",
     "start_time": "2017-11-13T23:03:29.145774Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "r6IlDIaacFS7"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\" \n",
    "    Takes in a corpus of documents and cleans. Needs multiple docs. \n",
    "    \n",
    "    IN: corpus of documents\n",
    "    \n",
    "    OUT: cleaned text = a list (documents) of lists (cleaned word in each doc)\n",
    "    \"\"\"\n",
    "\n",
    "    lemmizer = WordNetLemmatizer()\n",
    "\n",
    "    stop = ## TODO: import and/or create your list of stopwords\n",
    "   \n",
    "\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for doc in text:\n",
    "        cleaned_words = []\n",
    "        \n",
    "        # remove parentheticals\n",
    "        clean_parens = re.sub(r'\\([^)]*\\)', ' ', doc)\n",
    "        \n",
    "        \n",
    "        # tokenize into words\n",
    "        for word  in nltk.wordpunct_tokenize(clean_parens):  \n",
    "            low_word = word.lower()\n",
    "\n",
    "            # throw out any words in stop words (doing it here and later makes it faster)\n",
    "            if low_word not in stop:\n",
    "\n",
    "                # lemmatize  to roots\n",
    "                root_word = lemmizer.lemmatize(low_word)  \n",
    "\n",
    "                # keep if not in stopwords (yes, again)\n",
    "                ## TODO: remove stopwords again\n",
    "\n",
    "                    # put into a list of words for each document\n",
    "                    cleaned_words.append(root_word)\n",
    "        \n",
    "        # keep corpus of cleaned words for each document    \n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T23:10:23.734602Z",
     "start_time": "2017-11-13T23:09:47.312411Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1W4USidgcFS9",
    "nbpresent": {
     "id": "bccf0623-9d50-4348-b066-30e75b33538d"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_talks = clean_text(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T23:10:41.909418Z",
     "start_time": "2017-11-13T23:10:41.903706Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "t7crrhFEcFTA",
    "nbpresent": {
     "id": "88aaffe5-80d1-48a1-8f5d-de86860cdb0b"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: print a few of our cleaned words from talk 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIqfmvdscFTD",
    "nbpresent": {
     "id": "b2a8164d-36da-4244-a099-35e6a15a59e5"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: print a few of our cleaned words from talk 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nR9roy8CcFTI",
    "nbpresent": {
     "id": "fb04705f-3601-4f32-be67-947af848e523"
    }
   },
   "source": [
    "# Save \n",
    "Save as a pickle file (or csv) for topic modeling in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:44:35.528419Z",
     "start_time": "2017-11-13T20:44:35.373564Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DISb5LADcFTI",
    "nbpresent": {
     "id": "fef7aa92-d33e-41d4-a355-c6f96e28e71c"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/cleaned_talks.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(cleaned_talks, picklefile)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "f58Z1Kg6WyLV"
   ],
   "name": "ted_clean_jbcs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
