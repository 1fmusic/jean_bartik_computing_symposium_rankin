{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWi6v8VZcFR-",
    "nbpresent": {
     "id": "cc5e48d4-0c22-454b-94f6-59d0a2c24e44"
    }
   },
   "source": [
    "# NLP on Ted Talk transcripts\n",
    "\n",
    "slides and code located at\n",
    "\n",
    "https://github.com/1fmusic/jean_bartik_computing_symposium_rankin.git\n",
    "\n",
    "\n",
    "# Clean and Pre-process text\n",
    "In this notebook we will \n",
    "+ Load the data from a .csv file\n",
    "+ Tokenize\n",
    "+ Stem/lemmatize\n",
    "+ Normalize (remove stopwords, unwanted characters, punctuation, lowercase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new conda environment with the correct packages \n",
    "\n",
    "To create a new environment in (ana)conda - which you should do for each project so you dont break stuff - download the `environment.yml` file and follow these directions. \n",
    "\n",
    "Open a conda prompt (windows) or a terminal window (linux/mac):<br>\n",
    "            `$ cd /YOUR_PATH_TO_REPO/jean_bartik_computing_symposium_2020/`<br>\n",
    "            `$ conda env create -f environment.yml`<br>\n",
    "\n",
    "Activate the environment (the name is in the yml file)<br>\n",
    "            `$ conda activate jbcs2020`<br>\n",
    "            `$ jupyter notebook` \n",
    "\n",
    "Then click on the jupyter notebook titled `ted_clean_jbcs.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tooa5FNZXLGN"
   },
   "source": [
    "# Install and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T19:38:48.243042Z",
     "start_time": "2017-11-15T19:38:48.173542Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "J-gCJUeOcFSC",
    "nbpresent": {
     "id": "e1369d89-1774-4a4b-9832-5fc4ba50e389"
    },
    "outputId": "707d622f-b117-4fa6-ea81-d25bbab58bb1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY The first time you use the environment, download these packages from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T19:38:48.243042Z",
     "start_time": "2017-11-15T19:38:48.173542Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "J-gCJUeOcFSC",
    "nbpresent": {
     "id": "e1369d89-1774-4a4b-9832-5fc4ba50e389"
    },
    "outputId": "707d622f-b117-4fa6-ea81-d25bbab58bb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ada/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/ada/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/ada/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oxCGx8LcFSE",
    "nbpresent": {
     "id": "e731e3dc-c274-45d9-b74e-dfd6ad3c9ed8"
    }
   },
   "source": [
    "# Import Data\n",
    "We import the csv of transcripts and URLs into a pandas dataframe. \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html\n",
    "\n",
    "https://chrisalbon.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:02:51.838596Z",
     "start_time": "2017-11-13T20:02:50.971925Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yz_5f-0ncFSG",
    "nbpresent": {
     "id": "1e3ce296-d7f0-429f-a15b-2f81b4a24578"
    }
   },
   "outputs": [],
   "source": [
    "talks = pd.read_csv('./data/ted_trans.csv', encoding = \"UTF-8\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "qzBCEeKan2MS",
    "outputId": "eedb7c4c-9bea-4ddd-87cb-6305b35d0d13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         transcript  \\\n",
       "0           0  Good morning. How are you?(Laughter)It's been ...   \n",
       "1           1  Thank you so much, Chris. And it's truly a gre...   \n",
       "2           2  (Music: \"The Sound of Silence,\" Simon & Garfun...   \n",
       "3           3  If you're here today — and I'm very happy that...   \n",
       "4           4  About 10 years ago, I took on the task to teac...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...  \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...  \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...  \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows using pandas 'head()' method\n",
    "talks.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the transcript column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "talks = talks.loc[:,'transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHK6cMuwcFSQ",
    "nbpresent": {
     "id": "d55d24e6-6e57-4392-b749-a68a5979946f"
    }
   },
   "source": [
    "TODO: print portions from 3 different transcripts (**talks**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "QFRsbmEvcFSQ",
    "nbpresent": {
     "id": "8f751ba7-805e-4b90-aae1-3276a9e4d882"
    },
    "outputId": "748c59db-4477-41d3-9124-1b6ea094f5c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Good morning. How are you?(Laughter)It's been great, hasn't it? I've been blown away by the whole thing. In fact, I'm leaving.(Laughter)There have been three themes running through the conference which are relevant to what I want to talk about. One is the extraordinary evidence of human creativity in all of the presentations that we've had and in all of the people here. Just the variety of it and the range of it. The second is that it's put us in a place where we have no idea what's going to happen, in terms of the \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talks[0][:521]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO print talk 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksa5ULR0cFST"
   },
   "outputs": [],
   "source": [
    "##TODO print talk number 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y63-FRNAcFSW"
   },
   "outputs": [],
   "source": [
    "##TODO print another talk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poq8C6WOcFSZ",
    "nbpresent": {
     "id": "3b999ca7-9b38-495e-897e-553bc7f7855e"
    }
   },
   "source": [
    "#### number of transcripts you want to analyze (also creates a list of numbers for iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poq8C6WOcFSZ",
    "nbpresent": {
     "id": "3b999ca7-9b38-495e-897e-553bc7f7855e"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 2467)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileids = range(0,len(talks))\n",
    "fileids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWb53RsocFSh",
    "nbpresent": {
     "id": "4d2baf07-1299-42ef-9cd4-8152a1705473"
    }
   },
   "source": [
    "# Tokenize (split) into words\n",
    "Typically, you would just go straignt to word tokenization if you are planning to do topic modeling. There are MANY, MANY ways to tokenize text into words. I will just show a few, but feel free to explore the possibilities.\n",
    "\n",
    "## Method 1\n",
    "wordpunct_tokenize from NLTK\n",
    "splits the text into words and punctuaiton as separate tokens (this makes it easy to remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:05:30.057043Z",
     "start_time": "2017-11-13T20:05:29.977823Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "p2hHUkTvcFSi",
    "nbpresent": {
     "id": "29faa657-8c60-41dd-badd-8ef42b727c8d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pen\n",
      "-----\n",
      ",\n",
      "-----\n",
      "in\n",
      "-----\n",
      "terms\n",
      "-----\n",
      "of\n",
      "-----\n",
      "the\n",
      "-----\n",
      "future\n",
      "-----\n",
      ".\n",
      "-----\n",
      "No\n",
      "-----\n",
      "idea\n",
      "-----\n",
      "how\n",
      "-----\n",
      "this\n",
      "-----\n",
      "may\n",
      "-----\n",
      "play\n",
      "-----\n",
      "out\n",
      "-----\n",
      ".\n",
      "-----\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "tokenized_talks = [nltk.wordpunct_tokenize(talks[fileid]) \\\n",
    "             for fileid in fileids]\n",
    "\n",
    "#to view a few\n",
    "print('\\n-----\\n'.join(nltk.wordpunct_tokenize(talks[0][500:560])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "di7kGdGaLS1O"
   },
   "source": [
    "## Method 2\n",
    "Word_tokenize from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xL-IhFQ5otkj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pen\n",
      "-----\n",
      ",\n",
      "-----\n",
      "in\n",
      "-----\n",
      "terms\n",
      "-----\n",
      "of\n",
      "-----\n",
      "the\n",
      "-----\n",
      "future\n",
      "-----\n",
      ".\n",
      "-----\n",
      "No\n",
      "-----\n",
      "idea\n",
      "-----\n",
      "how\n",
      "-----\n",
      "this\n",
      "-----\n",
      "may\n",
      "-----\n",
      "play\n",
      "-----\n",
      "out.I\n"
     ]
    }
   ],
   "source": [
    "doc_words_word_tok = [nltk.word_tokenize(talks[fileid]) \\\n",
    "             for fileid in fileids]\n",
    "\n",
    "print('\\n-----\\n'.join(nltk.word_tokenize(talks[0][500:560])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pa3261ypcFSo",
    "nbpresent": {
     "id": "d2bade29-473c-431c-a8b9-e8a08f0f27b1"
    }
   },
   "source": [
    "# Normalization\n",
    "## Lemmatize\n",
    "\n",
    "+ A method for getting the word root.\n",
    "+ It will replace the ending with the correct letters instead of chopping it off like some of the stemming functions. This leaves us will a few non-stemmed words.  \n",
    "        i.e. children -> child,   capacities -> capacity, but also, unpredictability -> unpredictability\n",
    "        \n",
    "## Lowercase\n",
    "+ also lowercase using **.lower()** at the word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:07:23.996217Z",
     "start_time": "2017-11-13T20:06:44.952044Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Of27eaGOcFSp",
    "nbpresent": {
     "id": "f9e204aa-feb7-4869-a80a-ef3094215a45"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With with\n",
      "our our\n",
      "capabilities capability\n",
      ", ,\n",
      "we we\n",
      "will will\n",
      "educate educate\n",
      "the the\n",
      "children child\n",
      ". .\n",
      "They they\n",
      "are are\n",
      "all all\n",
      "associated associated\n",
      "with with\n",
      "various various\n",
      "playgrounds playground\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "lemmizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "my_text = \"With our capabilities, we will educate the children. They are all associated with various playgrounds.\"\n",
    "\n",
    "\n",
    "for word in nltk.wordpunct_tokenize(my_text):\n",
    "    print(word, lemmizer.lemmatize(word.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZDR_43mNHC4"
   },
   "source": [
    "## Stem\n",
    "Now we will see how stemming with the porter stemmer the tokenized words will cut off the word ending to get to the root. Now we get `recently -> recent`, but also `associated -> associ`.\n",
    "\n",
    "We can print out the original word next to the stemmed word to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXOAIwe0Na93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With with\n",
      "our our\n",
      "capabilities capabl\n",
      ", ,\n",
      "we we\n",
      "will will\n",
      "educate educ\n",
      "the the\n",
      "children children\n",
      ". .\n",
      "They they\n",
      "are are\n",
      "all all\n",
      "associated associ\n",
      "with with\n",
      "various variou\n",
      "playgrounds playground\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "\n",
    "my_text = \"With our capabilities, we will educate the children. They are all associated with various playgrounds.\"\n",
    "\n",
    "\n",
    "for word in nltk.wordpunct_tokenize(my_text):\n",
    "    print(word, stemmer.stem(word.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvHR7q7vcFSs",
    "nbpresent": {
     "id": "72c1add6-0d9e-4a13-9a3c-fe6982cbde25"
    }
   },
   "source": [
    "# Remove Stopwords, punctuation, or other non-letter/numbers\n",
    "+ NLTK has a set of common words that do not add any semantic information to our text, we will use this list and add our own items to it\n",
    "        + punctuation\n",
    "        + music notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibuQqobTcFSy",
    "nbpresent": {
     "id": "9e5c4811-1d57-44d9-afb6-a5b092aa60f7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = nltk.corpus.stopwords.words('english')\n",
    "stop[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbNtnp54cFS0",
    "nbpresent": {
     "id": "b74b3f51-371b-40ae-81a8-0a34d39939c2"
    }
   },
   "source": [
    "add our own terms or characters to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDCFJkR4cFS1",
    "nbpresent": {
     "id": "141b869c-74bc-4995-b27b-4a44d0393c64"
    }
   },
   "outputs": [],
   "source": [
    "stop += ['.', ',',':','...','!\"','?\"', \"'\", '\"',' - ',' — ',',\"','.\"',';','♫♫','♫']\n",
    "stop = set(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnJ2NwlocFS2",
    "nbpresent": {
     "id": "2533d426-28f1-4617-8db0-9a32431df333"
    }
   },
   "source": [
    "Write a function to remove the stop words from a document using our list. Print a few talks and see if there are still a few words in there that are not giving us any information. If so, add them to the **stop** list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phRRUgWGcFS4",
    "nbpresent": {
     "id": "ca884bd9-a8c3-4507-b307-41557024b15d"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IkYz6sNxcFSZ",
    "nbpresent": {
     "id": "6ad747ed-188f-426c-a325-cc496cf41043"
    }
   },
   "source": [
    "# Non-speech sounds, events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:04:46.891465Z",
     "start_time": "2017-11-13T20:04:46.883084Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "asDBpxnKcFSc",
    "nbpresent": {
     "id": "eeb71ea6-10a5-45ba-8f93-9f2b713a84f0"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful. I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night. And I say that sincerely, partly because   I need that. Put yourselves in my position. I flew on Air Force Two for eight years\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove parethetical non-speech sounds from text using a regular expression\n",
    "clean_parens_talks= [re.sub(r'\\([^)]*\\)', ' ', talks[fileid]) for fileid in fileids]\n",
    "\n",
    "# print one talk\n",
    "clean_parens_talks[1][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful. I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night. And I say that sincerely, partly because (Mock sob) I need that.(Laughter)Put yourselves in my position.(Laughter)I flew on Ai\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talks[1][:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kfz3tNyucFS6",
    "nbpresent": {
     "id": "053671ba-b59d-4142-9a04-1ab99329c1e0"
    }
   },
   "source": [
    "#  Define a cleaning function that combines the methods from above.\n",
    "1. clean (remove parentheticals)\n",
    "2. tokenize into words using wordpunct\n",
    "3. lowercase and remove stop words\n",
    "4. lemmatize or stem\n",
    "5. lowercase and remove stop words\n",
    "6. join the words back into a document and put into a list of cleaned documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T23:03:29.257095Z",
     "start_time": "2017-11-13T23:03:29.145774Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "r6IlDIaacFS7"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-f6c68e68492f>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-f6c68e68492f>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    stop = ## TODO: import and/or create your list of stopwords\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    \"\"\" \n",
    "    Takes in a corpus of documents and cleans. Needs multiple docs. \n",
    "    \n",
    "    IN: corpus of documents\n",
    "    \n",
    "    OUT: cleaned text = a list (documents) of lists (cleaned word in each doc)\n",
    "    \"\"\"\n",
    "\n",
    "    lemmizer = WordNetLemmatizer()\n",
    "\n",
    "    stop = ## TODO: import and/or create your list of stopwords\n",
    "   \n",
    "\n",
    "    cleaned_text = []\n",
    "    \n",
    "    for doc in text:\n",
    "        cleaned_words = []\n",
    "        \n",
    "        # remove parentheticals\n",
    "        clean_parens = re.sub(r'\\([^)]*\\)', ' ', doc)\n",
    "        \n",
    "        \n",
    "        # tokenize into words\n",
    "        for word  in nltk.wordpunct_tokenize(clean_parens):  \n",
    "            low_word = word.lower()\n",
    "\n",
    "            # throw out any words in stop words (doing it here and later makes it faster)\n",
    "            if low_word not in stop:\n",
    "\n",
    "                # lemmatize  to roots\n",
    "                root_word = lemmizer.lemmatize(low_word)  \n",
    "\n",
    "                # keep if not in stopwords (yes, again)\n",
    "                ## TODO: remove stopwords again\n",
    "\n",
    "                    # put into a list of words for each document\n",
    "                    cleaned_words.append(root_word)\n",
    "        \n",
    "        # keep corpus of cleaned words for each document    \n",
    "        cleaned_text.append(' '.join(cleaned_words))\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T23:10:23.734602Z",
     "start_time": "2017-11-13T23:09:47.312411Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1W4USidgcFS9",
    "nbpresent": {
     "id": "bccf0623-9d50-4348-b066-30e75b33538d"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_talks = clean_text(talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T23:10:41.909418Z",
     "start_time": "2017-11-13T23:10:41.903706Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "t7crrhFEcFTA",
    "nbpresent": {
     "id": "88aaffe5-80d1-48a1-8f5d-de86860cdb0b"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: print a few of our cleaned words from talk 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIqfmvdscFTD",
    "nbpresent": {
     "id": "b2a8164d-36da-4244-a099-35e6a15a59e5"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: print a few of our cleaned words from talk 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nR9roy8CcFTI",
    "nbpresent": {
     "id": "fb04705f-3601-4f32-be67-947af848e523"
    }
   },
   "source": [
    "# Save \n",
    "Save as a pickle file (or csv) for topic modeling in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T20:44:35.528419Z",
     "start_time": "2017-11-13T20:44:35.373564Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DISb5LADcFTI",
    "nbpresent": {
     "id": "fef7aa92-d33e-41d4-a355-c6f96e28e71c"
    }
   },
   "outputs": [],
   "source": [
    "with open('./data/cleaned_talks.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(cleaned_talks, picklefile)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "f58Z1Kg6WyLV"
   ],
   "name": "ted_clean_jbcs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
