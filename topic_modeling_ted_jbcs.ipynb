{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRO91ndzIOu5"
   },
   "source": [
    "# Bag Of Words Topic Modeling and Recommender for Ted Talks - JBCS\n",
    " - Summer K. Rankin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUnwZ5kZIOu9"
   },
   "source": [
    "once the text has been cleaned, we move onto the next steps\n",
    "        - 1. Vectorize \n",
    "        - 2. Topic modeling\n",
    "        - 3. Visualization\n",
    "        - 4. Recommender (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfJWdiQsIOu_"
   },
   "source": [
    "# Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:08:30.547215Z",
     "start_time": "2017-11-15T20:08:28.639903Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3CeRpnv3IOvB"
   },
   "outputs": [],
   "source": [
    "import nltk, re, pickle, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation,  TruncatedSVD, NMF\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing  import  StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyLDAvis\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fc7XJO0ZYh4D"
   },
   "source": [
    "# Import the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:08:32.451011Z",
     "start_time": "2017-11-15T20:08:31.800142Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tvwZHlqwIOvN"
   },
   "outputs": [],
   "source": [
    "with open('./data/cleaned_talks.pkl', 'rb') as picklefile:\n",
    "    cleaned_talks = pickle.load(picklefile)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xRVKlSnd4WUB"
   },
   "source": [
    "# 1 Vectorization\n",
    "Vectorization is the important step of turning our words into numbers. There are 2 common methods: count vectorizer, tf-idf. This function takes each word in each document and counts the number of times the word appears. You end up with each word (and n-gram) as your columns and each row is a document, so the data is the frequency of each word in each document. As you can imagine, there will be a large number of zeros in this matrix; we call this a sparse matrix. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "tf-IDF\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "----\n",
    "\n",
    "For this tutorial the tokenization and vectorization gets bundled together because we are using skleanrn's feature extraction functions. This means we will set the parameters of these functions to tokenize the way we want, include n-grams, and set thresholds for max or min document frequency of a term. https://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af\n",
    "\n",
    "- takes us from words to numbers \n",
    "- create the document-term matrix which is the basis for all modeling\n",
    "    - row = document, column = word or n-gram, data = word's weight for that document\n",
    "- we will vectorize in 2 ways \n",
    "    1. counting the frequency of each term in each document (**CountVectorizer**)\n",
    "    2. counting the frequency of each term in each document and weighting by the number of times the term appears in the corpus. Term Frequency * Inverse Document Frequency (**TfidfVectorizer**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjhVfWQbIOvX"
   },
   "source": [
    "# 1.1 Count Vectorize \n",
    "+ Using Sklearn algorithms with text data\n",
    "+ CountVectorizer: Convert a collection of text documents to a matrix of token counts \n",
    "+ This implementation produces a sparse representation\n",
    "+  **CountVectorizer** is a class; so **vectorizer** below represents an instance of that object\n",
    "+ note that we can also **lowercase** , remove **stopwords** and search for a certain pattern **token_pattern** (i.e. letters only) by using the parameters of this function. \n",
    "+ a specific vocabulary can also be passed to this function. \n",
    "+ for more info about the many parameters, see the sklearn docs. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "CountVectorizer is a class; so `vectorizer` below represents an instance of that object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "colab_type": "code",
    "id": "eHvDuH-VNipZ",
    "outputId": "502efa9c-5475-49f5-fb93-adef41bb684f"
   },
   "outputs": [],
   "source": [
    "c_vectorizer = CountVectorizer(ngram_range=(1,3), \n",
    "                             stop_words='english', \n",
    "                             max_df = 0.6, \n",
    "                             max_features=2000)\n",
    "\n",
    "# call `fit` to build the vocabulary and calculate the weights\n",
    "c_vectorizer.fit(cleaned_talks)\n",
    "\n",
    "# finally, call `transform` to apply the weights and convert text to a bag of words\n",
    "count_vect_data = c_vectorizer.transform(cleaned_talks)\n",
    "\n",
    "# to view the document-term matrix, we can transpose back to a dense array\n",
    "pd.DataFrame(data = count_vect_data.toarray(), columns=sorted(c_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgwMTjtNIOvs"
   },
   "source": [
    "# 1.2 Tfidf (term frequency * inverse document frequency)\n",
    "+ Assign a weight to each term in each document rather than a raw count. \n",
    "+ gives more weight to less frequent terms\n",
    "+ similar paremeters for this vectorizer \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-10T02:12:53.508282Z",
     "start_time": "2017-11-10T02:11:40.354134Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GBgRQBsYIOvu"
   },
   "outputs": [],
   "source": [
    "t_vectorizer = TfidfVectorizer(ngram_range=(1, 3),  \n",
    "                                   stop_words='english', \n",
    "                                   token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                                   lowercase=True,\n",
    "                                   max_df = 0.6,\n",
    "                                   max_features=2000)\n",
    "\n",
    "\n",
    "# call `fit` to build the vocabulary and calculate weights\n",
    "t_vectorizer.fit(cleaned_talks)\n",
    "\n",
    "# finally, call `transform` to convert text to a bag of words\n",
    "tfidf_data = t_vectorizer.transform(cleaned_talks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "colab_type": "code",
    "id": "XzXTTSrIcleZ",
    "outputId": "5686768c-7729-4b52-8f51-19d6b9cd997f"
   },
   "outputs": [],
   "source": [
    "# view a dense representation of the document-term matrix\n",
    "pd.DataFrame(tfidf_data.toarray(), columns=t_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qthRkfO4R_M"
   },
   "source": [
    "# 2 Topic modeling \n",
    "Use the document term matrix created with vectorization, to create a latent space and find the words that tend to ocurr together\n",
    "\n",
    "We will use LDA Latent Dirichlet Allocation here (there are several methods, NMF, SVD)\n",
    "\n",
    "This will reduce the data from thousands of terms (dimensions) to 20 topics. (Dimensionality Reduction)\n",
    "\n",
    "Creates a latent space that is X dimensions.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WY1cESM2IOv1"
   },
   "source": [
    "# LDA Latent Dirichlet Allocation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:06:54.880541Z",
     "start_time": "2017-11-15T20:06:54.812697Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "d4lXxWUFIOv5"
   },
   "outputs": [],
   "source": [
    "def topic_mod(vectorizer, vect_data, topics=20, iters=5, no_top_words=50):\n",
    "    \n",
    "    \"\"\" use Latent Dirichlet Allocation to get topics\"\"\"\n",
    "\n",
    "    mod = LatentDirichletAllocation(n_components=topics,\n",
    "                                    max_iter=iters,\n",
    "                                    random_state=42,\n",
    "                                    learning_method='online',\n",
    "                                    n_jobs=-1)\n",
    "    \n",
    "    mod_dat = mod.fit_transform(vect_data)\n",
    "    \n",
    "    \n",
    "    # to display a list of topic words and their scores \n",
    "    \n",
    "    def display_topics(model, feature_names, no_top_words):\n",
    "        for ix, topic in enumerate(model.components_):\n",
    "            print(\"Topic \", ix)\n",
    "            print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]) + '\\n')\n",
    "    \n",
    "    display_topics(mod, vectorizer.get_feature_names() , no_top_words)\n",
    "\n",
    "    \n",
    "    return mod, mod_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmmdftaFXaZ_"
   },
   "outputs": [],
   "source": [
    "lda_model, lda_data = topic_mod(c_vectorizer, \n",
    "                            count_vect_data, \n",
    "                            topics=20, \n",
    "                            iters=10, \n",
    "                            no_top_words=15)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA-bPJRAIOwq"
   },
   "source": [
    "# 3 Visualization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arFh_8_s4m_Y"
   },
   "source": [
    "# 3.1 View distribution of topics with pyLDAvis \n",
    "+ plot the first 2 components from the topic modeling (LDA). \n",
    "+ Not really the best way to look at clusters, but a good place to start and a very nice way to present data to clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:28:17.621582Z",
     "start_time": "2017-11-15T20:28:09.867331Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "fyYrDYoFIOwu",
    "outputId": "87626c1b-551b-41a1-976e-7773ada9c0a3"
   },
   "outputs": [],
   "source": [
    "# Setup to run in Jupyter notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    " # Create the visualization\n",
    "vis = pyLDAvis.sklearn.prepare(lda_model, count_vect_data, c_vectorizer, sort_topics=False, mds='mmds')\n",
    "\n",
    " # can export as a standalone HTML web page\n",
    "pyLDAvis.save_html(vis, 'lda_visual.html')\n",
    "\n",
    "# # Let's view it!\n",
    "display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:25:46.602136Z",
     "start_time": "2017-11-15T20:25:46.598326Z"
    },
    "colab_type": "text",
    "id": "fwHOYf2aIOwF"
   },
   "source": [
    "# 3.2 Assign a topic to each document\n",
    "\n",
    "+ for each document, assign the topic (column) with the  highest score from the LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:25:00.419658Z",
     "start_time": "2017-11-15T20:25:00.388311Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "W9Bi_CGyIOwI"
   },
   "outputs": [],
   "source": [
    "topic_index = np.argmax(lda_data, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Yrdyk1v6Ws-"
   },
   "source": [
    " # 3.3 Assign labels to topics\n",
    " Try to use the top terms from each topic to give it a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T22:16:11.778984Z",
     "start_time": "2017-11-13T22:16:11.624198Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MZqdbTMqIOwR"
   },
   "outputs": [],
   "source": [
    "topic_names = pd.DataFrame(topic_index)\n",
    "\n",
    "topic_names[topic_names==0] = \"family\"\n",
    "topic_names[topic_names==1] = \"agriculture\"\n",
    "topic_names[topic_names==2] = \"space\"\n",
    "topic_names[topic_names==3] = \"environment\"\n",
    "topic_names[topic_names==4] = \"global economy\"\n",
    "topic_names[topic_names==5] = \"writing\"\n",
    "topic_names[topic_names==6] = \"sounds\"\n",
    "topic_names[topic_names==7] = \"belief, mortality\"\n",
    "topic_names[topic_names==8] = \"transportation\"\n",
    "\n",
    "topic_names[topic_names==9] = \"gaming\"\n",
    "topic_names[topic_names==10] = \"architecture\"\n",
    "topic_names[topic_names==11] = \"education\"\n",
    "\n",
    "topic_names[topic_names==12] = \"neuroscience\"\n",
    "topic_names[topic_names==13] = \"climate, energy\"\n",
    "\n",
    "topic_names[topic_names==14] = \"politics\"\n",
    "topic_names[topic_names==15] = \"robotics\"  \n",
    "topic_names[topic_names==16] = \"disease biology\"\n",
    "topic_names[topic_names==17] = \"medicine\"\n",
    "topic_names[topic_names==18] = \"technology, privacy\"\n",
    "topic_names[topic_names==19] = \"war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T22:16:30.403758Z",
     "start_time": "2017-11-13T22:16:30.368886Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "VhEt8znVIOwb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FgnG9yjO1dJ9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iy1YLOKS1yFt"
   },
   "source": [
    "# 4. Recommender (optional)\n",
    "we will use the Ted Talk metadata to add some information to our recommender. \n",
    "\n",
    "# import original talks and metadata\n",
    "merge them on the 'url' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "mBjJsRug1dSi",
    "outputId": "2da4442b-f4d7-4234-d3ab-f3b909771727"
   },
   "outputs": [],
   "source": [
    "ted_trans = pd.read_csv('/data/transcripts.csv', encoding = \"UTF-8\")  \n",
    "ted_main = pd.read_csv('/data/ted_main.csv', encoding = \"UTF-8\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Tyiept22OuS"
   },
   "outputs": [],
   "source": [
    "ted_all = pd.merge(ted_trans, right=ted_main, on='url')\n",
    "ted_all.url = ted_all.url.astype('str',copy=False)\n",
    "\n",
    "ted_all.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-13T22:17:07.519083Z",
     "start_time": "2017-11-13T22:17:07.481396Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "mvw0Da3NIOwj"
   },
   "outputs": [],
   "source": [
    "def get_recommendations(target_doc, num_of_recs, topics, data, topic_model, vectorizer, topic_model_data):\n",
    "    \n",
    "    new_vec = topic_model.transform(\n",
    "        vectorizer.transform([target_doc]))\n",
    "    \n",
    "    nn = NearestNeighbors(n_neighbors=num_of_recs, metric='cosine', algorithm='brute')\n",
    "    nn.fit(topic_model_data)\n",
    "    \n",
    "    results = nn.kneighbors(new_vec)\n",
    "    \n",
    "    recommend_list = results[1][0]\n",
    "    scores = results[0]\n",
    "                       \n",
    "    ss = np.array(scores).flat\n",
    "    for i, resp in enumerate(recommend_list):\n",
    "        print('\\n--- ID ---\\n', + resp)\n",
    "        print('--- distance ---\\n', + ss[i])  \n",
    "        print('--- topic ---')\n",
    "        print(topics.iloc[resp,0])\n",
    "        print(data.iloc[resp,1])\n",
    "        print('--- teds tags ---')\n",
    "        print(data.iloc[resp,-3])\n",
    "        \n",
    "    return recommend_list, ss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuDD5J5u1UYt"
   },
   "outputs": [],
   "source": [
    "rec_list, scores = get_recommendations(cleaned_talks[804], num_of_recs=10, topics=topic_names, data=ted_all,\n",
    "                                       topic_model=lda_model, vectorizer=c_vectorizer, topic_model_data=lda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9AE8sG44Rht"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hic3xHc841Jb"
   },
   "source": [
    "# 4.1 search and recommend similar documents\n",
    "We use a great library called fuzzywuzzy to find the titles that is most similar to a search term. then we use this as our target document for the recommendation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "Ic7iyyjC5IZ8",
    "outputId": "a9a336ab-a1e4-4c67-cd10-08c96d3037b8"
   },
   "source": [
    "install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "7Vws5xfs4Rmi",
    "outputId": "bd466379-2fa9-44f8-97da-51dde3bcf745"
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "search_term = \"computer science\"\n",
    "\n",
    "titles = ted_all['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYq1ofsY4RrH"
   },
   "outputs": [],
   "source": [
    "tite, score, talk_ind = process.extractOne(search_term, titles, scorer=fuzz.token_set_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfpdIc8f4p6c"
   },
   "outputs": [],
   "source": [
    "rec_list, scores = get_recommendations(cleaned_talks[talk_ind], num_of_recs=10, topics=topic_names, data=ted_all,\n",
    "                                       topic_model=lda_model, vectorizer=c_vectorizer, topic_model_data=lda_data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of topic_modeling_ted_jbcs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
